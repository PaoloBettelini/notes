\documentclass[preview]{standalone}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{parskip}
\usepackage{stellar}
\usepackage{definitions}
\usepackage{bettelini}

\begin{document}

% vector space

\begin{snippetdefinition}{linearalgebra-vector-definition}{Vector}
    A vector is a geometric object that has a direction and a magnitude. \\
    Vectors don't have an origin point and they can be represented in any position on the space.

    A vector can be expressed with its components

    \[
        \vec{a} =
        \begin{pmatrix}
            x \\
            y \\
            z
        \end{pmatrix}
    \]

    For an n-dimensional vector

    \[
        \vec{a} =
        \begin{pmatrix}
            \vec{a}_1 \\
            \vec{a}_2 \\
            \vdots \\
            \vec{a}_n
        \end{pmatrix},
        \quad \vec{a} \in {\realnumbers}^n
    \]
\end{snippetdefinition}

\begin{snippetdefinition}{linearalgebra-linear-combination-definition}{Linear Combination}
    A linear combination is a sum of two or more vectors, each with a coefficient.

    \[
        \vec{c} = a \cdot\vec{a} + b\cdot\vec{b},
        \quad a,b\in \realnumbers
    \]
\end{snippetdefinition}

\begin{snippetdefinition}{linearalgebra-span-definition}{Span}
    The \textit{span} of a set of vectors, is the set of every possible linear combination.
\end{snippetdefinition}

\begin{snippetdefinition}{matrix-definition}{Matrix}
    A matrix is a rectangular array of elements.
    Each matrix has a particular size expressed as \(n \times m\).
    A generic matrix looks like the following

    \[
    A = \begin{bmatrix} 
            a_{1,1} & a_{1,2} & \cdots & a_{1,m} \\
            a_{2,1} & a_{2,2} & \cdots & a_{2,m} \\
            \vdots  & \vdots  & \ddots & \vdots  \\
            a_{n,1} & a_{n,2} & \cdots & a_{n,m} 
        \end{bmatrix}
    \]
\end{snippetdefinition}

\begin{snippetdefinition}{linearalgebra-elementary-row-operations}{Elementary row operations}
    \begin{itemize}
        \item \textbf{Elementary row switching}:
        To switch two rows of a matrix.
        \item \textbf{Elementary row multiplication}:
        To multiply each element of a row by
        a scalar \(k \neq 0\).
        \item \textbf{Elementary row addition}:
        To add a multiple of every element of a row
        to each respective element of a another row.
    \end{itemize}

\end{snippetdefinition}

\begin{snippetproposition}{linearalgebra-lin-sys-sol-amount}{Solutions of a system of linear equations}
    A system of linear equations can have \(0\), \(1\)
    or an infinite amount of solutions.
\end{snippetproposition}

\begin{snippetexample}{linearalgebra-matrix-linear-system-1-sol-example-1}{Matrix linear system 1 solution}
    \[
        \begin{bmatrix}
            1 & 1 & 1 & 2 \\
            3 & 1 & -2 & -2 \\
            2 & 4 & 1 & 0
        \end{bmatrix}
        \rightarrow
        \begin{bmatrix}
            1 & 0 & 0 & 1 \\
            0 & 1 & 0 & -1 \\
            0 & 0 & 1 & 2
        \end{bmatrix}
    \]
\end{snippetexample}

\begin{snippetexample}{linearalgebra-matrix-linear-system-inf-sol-example-1}{Matrix linear system infinite solutions}
    \[
        \begin{bmatrix}
            2 & -1 & 1 & 3 & 2 \\
            2 & -1 & 1 & 2 & 4 \\
            4 & -3 & 3 & 8 & 8
        \end{bmatrix}
        \rightarrow
        \begin{bmatrix}
            1 & 0 & 0 & -1 & 2 \\
            0 & 1 & -1 & -4 & 0 \\
            0 & 0 & 0 & 0 & 0
        \end{bmatrix}
    \]
\end{snippetexample}

\begin{snippetexample}{linearalgebra-matrix-linear-system-0-sol-example-1}{Matrix linear system 0 solutions}
    \[
        \begin{bmatrix}
            1 & 1 & 1 & 1 & 4 \\
            2 & 3 & -2 & -3 & 1 \\
            1 & 0 & 5 & 6 & 1
        \end{bmatrix}
        \rightarrow
        \begin{bmatrix}
            1 & 0 & -5 & -6 & -11 \\
            0 & 1 & -4 & -9 & -7 \\
            0 & 0 & 0 & 0 & 1
        \end{bmatrix}
    \]
\end{snippetexample}

\begin{snippetdefinition}{linearalgebra-zero-matrix-definition}{Zero matrix}
    The zero matrix, denoted \(0\) or \(0_{m,n}\)
    is defined as the matrix of size \((m \times n)\)
    where every element is \(0\).
\end{snippetdefinition}

\begin{snippetdefinition}{linearalgebra-matrix-addition-definition}{Matrix addition}
    Two matrices \(A\) and \(B\) of the same size
    with elements \(a_{i,j}\) and \(b_{i,j}\) respectively
    can be added. The resulting matrix \(A+B=C\)
    has elements \(c_{i,j} = a_{i,j} + b_{i,j}\).
\end{snippetdefinition}

\begin{snippetdefinition}{linearalgebra-matrix-scalar-multiplication-definition}{Matrix scalar multiplication}
    A matrix \(A\) with elements \(a_{i,j}\)
    can be multiplied by a scalar \(k\).
    The resulting matrix \(kA=B\) has elements
    \(b_{i,j} = k \cdot a_{i,j}\).
\end{snippetdefinition}

\begin{snippetdefinition}{linearalgebra-matrix-multiplication-definition}{Matrix multiplication}
    Two matrices \(A\) of size \((m \times n)\) and
    \(B\) of size \((p \times q)\)
    with elements \(a_{i,j}\) and \(b_{i,j}\) respectively
    can be multiplied if \(n=p\).
    The resulting matrix \(AB=C\) has elements
    \[
        c_{i,j} = \sum_{k=1}^n
        a_{i,k}b_{k,j}
    \]
    with \(1 \leq i \leq m\)
    and \(1 \leq j \leq q\).
\end{snippetdefinition}

\begin{snippetdefinition}{linearalgebra-kronecker-delta-definition}{Kronecker delta}
    The \textit{Kronecker delta} is a \function of two variables
    defined as follows:
    \[
        \delta_{i,j} = \begin{cases}
            0 & i \neq j \\
            1 & i = j
        \end{cases}
    \]
\end{snippetdefinition}

\begin{snippetdefinition}{linearalgebra-identity-matrix-definition}{Identity matrix}
    The \textit{Identity matrix}, denoted \(I_n\),
    is a square matrix of side \(n\)
    with entries \(\delta_{i,j}\).
    \[
    I_n=
        \begin{bmatrix}
            1 & 0 & 0 & \cdots & 0 \\
            0 & 1 & 0 & \cdots & 0 \\
            0 & 0 & 1 & \cdots & 0 \\
            \vdots & \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & 0 & \cdots & 1 \\
        \end{bmatrix}
    \]
\end{snippetdefinition}

\begin{snippetproposition}{linearalgebra-kronecker-delta-sifting-property}{Kronecker delta sifting property}
    Let \(x_1, x_2, \cdots, x_n \in \realnumbers\)
    and \(1 \leq k \leq n\).
    \[
        \sum_{i=1}^n \delta_{i,k}x_i = x_k
    \]
\end{snippetproposition}

\begin{snippetdefinition}{linearalgebra-matrix-premultiplication}{Matrix premultiplication}
    A matrix \(M\) can be \textit{premultiplied}
    by another matrix \(A\), resuling in \(AM\).
\end{snippetdefinition}

\begin{snippetdefinition}{linearalgebra-matrix-postmultiplication}{Matrix postmultiplication}
    A matrix \(M\) can be \textit{postmultiplied}
    by another matrix \(A\), resulting in \(MA\).
\end{snippetdefinition}

\begin{snippetdefinition}{linearalgebra-exponentiation-definition}{Matrix exponentiation}
    Given a square matrix \(M\), the power of a matrix
    is defin as
    \[
        M^k = \underbrace{M\cdot M \cdot M \cdots M}_{k \text{ times}}
    \]
\end{snippetdefinition}

\begin{snippetdefinition}{linearalgebra-polynomial-of-a-matrix-definition}{Polynomial of a matrix}
    Given a polynomial \(p(x)=\sum_{n=0}^k a_nx^n\)
    and a square matrix \(M\) we defined
    \[
        p(M) = \sum_{n=0}^k a_nM^n
    \]
\end{snippetdefinition}

\begin{snippetdefinition}{linearalgebra-matrix-exponentiation-definition}{Matrix Exponentiation}
    TODO
\end{snippetdefinition}

\begin{snippetexample}{linearalgebra-matrix-exponentiation-example-1}{Matrix exponentiation example 1}
    \begin{align*}
        & {\begin{bmatrix}
            \cos\alpha & \sin\alpha \\
            \sin\alpha & -\cos\alpha
        \end{bmatrix}}^2
        = \begin{bmatrix}
            \cos^2\alpha + \sin^2\alpha & \cos\alpha\sin\alpha - \sin\alpha\cos\alpha \\
            \cos\alpha\sin\alpha - \sin\alpha\cos\alpha & \cos^2\alpha + \sin^2\alpha
        \end{bmatrix}
        \\
        &= \begin{bmatrix}
            1 & 0 \\
            0 & 1
        \end{bmatrix} = I_2
    \end{align*}
    For all \(\alpha \in \realnumbers\)
\end{snippetexample}

\begin{snippetexample}{linearalgebra-matrix-exponentiation-example-2}{Matrix exponentiation example 2}
    \(\nexists A \suchthat A^2 = B\) where
    \[
        B = \begin{bmatrix}
            0 & 1 \\
            0 & 0
        \end{bmatrix}
    \]
    To prove this, let \(a,b,c,d \in \complexnumbers\)
    \[
        \begin{bmatrix}
            0 & 1 \\
            0 & 0
        \end{bmatrix}
        =
        {\begin{bmatrix}
            a & b \\
            c & d
        \end{bmatrix}}^2
        =
        \begin{bmatrix}
            a^2+bc & b(a+d) \\
            c(a+d) & bc+d^2
        \end{bmatrix}
    \]
    Since \(c(a+d)=0\), then \(c=0 \land a+d=0\).
    But this contradicts \(b(a+d)=1\) meaning \(a+d \neq 0\).
\end{snippetexample}

\begin{snippetproposition}{linearalgebra-matrix-identity-postmultiplication}{Matrix Identity Postmultiplication}
    Given a matrix \(A\),
    \[ AI_n=A \]
\end{snippetproposition}

\begin{snippetproof}{linearalgebra-matrix-identity-postmultiplication-proof}{linearalgebra-matrix-identity-postmultiplication}{Matrix Identity Postmultiplication}
    Let the matrix \(A\) be defined with the
    elements \(a_{i,j}\).
    Then, by the sifting property % TODO link
    \[
        a_{i,j} = \sum_{k=1}^n
        a_{i,j}\delta_{k,j} = a_{i,j}
    \]
\end{snippetproof}

\begin{snippetproposition}{linearalgebra-matrix-identity-premultiplication}{Matrix Identity Premultiplication}
    Given a matrix \(A\),
    \[ I_n A=A \]
\end{snippetproposition}

\begin{snippetproof}{linearalgebra-matrix-identity-premultiplication-proof}{linearalgebra-matrix-identity-premultiplication}{Matrix Identity Premultiplication}
    Let the matrix \(A\) be defined with the
    elements \(a_{i,j}\).
    Then, by the sifting property % TODO link
    \[
        a_{i,j} = \sum_{k=1}^n
        \delta_{i,k}a_{k,j} = a_{i,j}
    \]
\end{snippetproof}

\begin{snippetdefinition}{linearalgebra-matrix-inverse-definition}{Matrix Inverse}
    Let \(M\) be a square matrix of side \(n\).
    A matrix \(B\) is an \textit{inverse}
    of \(M\) if \[BM=MB=I_n\]
    The inverse of \(M\) is denoted by \(M^{-1}\).
\end{snippetdefinition}

\begin{snippetdefinition}{linearalgebra-invertible-matrix-definition}{Invertible Matrix}
    A matrix is \textit{invertible} if it has an inverse.
    Otherwise, it is \textit{singular}.
\end{snippetdefinition}

\begin{snippetproposition}{linearalgebra-uniqueness-of-matrix-inverse}{Uniqueness of matrix inverse}
    If a matrix \(A\) has an inverse, then it is unique.
\end{snippetproposition}

\begin{snippetproof}{linearalgebra-uniqueness-of-matrix-inverse-proof}{linearalgebra-uniqueness-of-matrix-inverse}{Uniqueness of matrix inverse}
    Suppose a matrix \(A\) has distinct inverses \(B\)
    and \(C\).
    It is possible to show that \(B\) and \(C\) are actually
    the same matrix.
    \[
        C=I_n C = (BA) C = B (AC) = BI_n = B
    \]
\end{snippetproof}

\begin{snippetproposition}{linearalgebra-involution-rule-for-matrix-inverse}{Involution rule for matrix inverse}
    If \(M\) is an invertible matrix
    \[
        {(M^{-1})}^{-1} = M
    \]
\end{snippetproposition}

\begin{snippetproof}{linearalgebra-involution-rule-for-matrix-inverse-proof}{linearalgebra-involution-rule-for-matrix-inverse}{Involution rule for matrix inverse}
    Since \(M(M^{-1}) = I\), \({(M^{-1})}^{-1}\)
    by uniqueness. % TODO link
\end{snippetproof}

\begin{snippetproposition}{linearalgebra-product-rule-of-inverse-matrices}{Product rule of inverse matrices}
    For invertible matrices \(A\) and \(B\)
    \[
        {(AB)}^{-1} = A^{-1} B^{-1}
    \]
\end{snippetproposition}

\begin{snippetproof}{linearalgebra-product-rule-of-inverse-matrices-proof}{linearalgebra-product-rule-of-inverse-matrices}{Product rule of inverse matrices}
    TODO
\end{snippetproof}

\begin{snippetdefinition}{linearalgebra-matrix-left-and-write-inverses}{Matrix left and write inverses}
    Let \(A\) is an \(m \times n\) matrix. A matrix \(B\)
    is called a \textit{right inverse} of \(A\)
    if \(AB=I_m\).
    A matrix \(B\) is called a \textit{right inverse} of \(A\)
    if \(BA=I_n\).
\end{snippetdefinition}

\begin{snippetproposition}{linearalgebra-inverse-of-a-non-square-matrix}{Inverse of a non-square matrix}
    If \(A\) is a non-square matrix, it cannot have
    both a \textit{left inverse} and a \textit{right inverse}.
\end{snippetproposition}

\begin{snippetproposition}{linearalgebra-inverse-of-a-square-matrix}{Inverse of a square matrix}
    If \(A\) is a square matrix
    with a left inverse \(B\),
    then \(B\) is also a right inverse.
\end{snippetproposition}

%\begin{snippetproposition}{linearalgebra-matrix-invertibility}{Matrix invertibility}
%    A matrix \(M\) is invertible iff
%    \(\det(M) \neq 0\).
%\end{snippetproposition}

\end{document}