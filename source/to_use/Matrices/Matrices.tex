\documentclass[a4paper]{article}

\usepackage{amsmath,stackengine}
\usepackage{amssymb}
\usepackage{parskip}
\usepackage{fullpage}
\usepackage{hyperref}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    urlcolor=blue,
    pdftitle={Matrices},
    pdfpagemode=FullScreen,
}

\title{Matrices}
\author{Paolo Bettelini}
\date{}

\newcommand{\braket}[1]{\left\langle#1\right\rangle}
\newcommand{\innerprod}[2]{\braket{#1\,|\,#2}}

\stackMath{}

\begin{document}

\maketitle
\tableofcontents
\pagebreak

\section{Definition}

A matrix is a rectangular array of elements.
Each matrix has a particular size expressed as \(n \times m\).
A generic matrix looks like the following

\[
A = \begin{bmatrix} 
        a_{1,1} & a_{1,2} & \cdots & a_{1,m} \\
        a_{2,1} & a_{2,2} & \cdots & a_{2,m} \\
        \vdots  & \vdots  & \ddots & \vdots  \\
        a_{n,1} & a_{n,2} & \cdots & a_{n,m} 
    \end{bmatrix}
\]

\section{Linear Transformation}

\subsection{Definition}

Multiplying a vector by a matrix produces another vector. This is a linear transformation. \\
A linear transformation \(T\) moves a vector around the vector space and/or to another space.

Applying a \textit{linear} transformation doesn't move the origin of the space, and every line
is kept linear. This operation can stretch and rotate every vector on the plane.
The matrix contains the information about this transformation.

\[
    \mathcal{V} \xrightarrow{T} \mathcal{W}
\]

The linear transformation has the following proprierties:

\begin{enumerate}
    \item \(T(c\vec{a})=cT\vec{a}\)
    \item \(T(\vec{a} + \vec{b}) = T\vec{a} + T\vec{b}\)
\end{enumerate}

where \(c\) is a scalar of the vector space.

\subsection{The basis}

Given a basis for a vector space

\[
    \mathcal{B}=\{\vec{b}_1, \vec{b}_2, \ldots, \vec{b}_n\}
\]

we can expand a vector \(\vec{a}\) along this basis

\[
    \vec{a} = \sum_{k=1}^{n} \alpha_k \mathcal{B}_k
\]

We then apply a transformation \(T\) to the vector \(\vec{a}\) and use the proprierties of
linear transformations

\[
    T\vec{a}
    = T\sum_{k=1}^{n} \alpha_k \mathcal{B}_k
    = \sum_{k=1}^{n} \alpha_k T\mathcal{B}_k
\]

This tells us that we only need \(T\mathcal{B}=\{T\vec{b}_1, T\vec{b}_2, \ldots, T\vec{b}_n\}\)
to determine \(T\) applied to any vector on the vector space.
Any transformation \(T\) is completely described by \(\mathcal{B}\)
and \(T\mathcal{B}\).

Each column of a matrix is indeed the result of applying its transformation
to the corresponding vector in the basis.
Intuitively, this is given by the fact that we only need to know where the vectors of the basis
end up after the transformation in order to represent the whole information.


% Add matrix
%
% i_1   j_1
% i_2   j_2
%
% Ti    Tj
% youtube kYB8IZa5AuE

\pagebreak

\section{Operations with matrices}

\subsection{Matrix multiplication}

Matrix multiplication forms a composition of multiple linear transformations. \\
This operation is not a communitative operation.

\[
    AB\neq BA
\]

However it is an associative operation.

\[
    A(BC)=(AB)C
\]

Matrix multiplication can be computed only if the numer of columns of the first matrix is equal to the number of rows of the other matrix. \\
This menas that we can compute the product of a \(n \times m\) matrix with a \(i \times j\) matrix, only if \(m=i\). \\
The resulting product is a matrix with \(n\) rows and \(j\) columns.

\[
    (n \times m) \cdot (m \times j) = (n \times j)
\]

If we multiply a \(1 \times n\) matrix by a \(n \times 1\) one, the result is a \(1 \times 1\) matrix or a scalar.

\[
    \begin{bmatrix} 
        a_1 & a_2 & \cdots & a_n
    \end{bmatrix}
    \cdot
    \begin{bmatrix} 
        b_1 \\
        b_2 \\
        \vdots \\
        b_n
    \end{bmatrix}
    = \sum_{k=1}^{n} a_k b_k = a_1 b_1 + a_2 b_2 + \cdots + a_n b_n
\]

This operation is pretty much the same as the dot product in vectors.

For bigger matrices, we apply the same `dot product' between every row and every column. \\
Here is an example:

\[
    \begin{bmatrix} 
        a_1 && a_2 && a_3 && a_4 \\
        \mathbf{b_1} && \mathbf{b_2} && \mathbf{b_3} && \mathbf{b_4} \\
        c_1 && c_2 && c_3 && c_4
    \end{bmatrix}
    \cdot
    \begin{bmatrix} 
        d_1 && \mathbf{e_1} \\
        d_2 && \mathbf{e_2} \\
        d_3 && \mathbf{e_3} \\
        d_4 && \mathbf{e_4}
    \end{bmatrix}
    =
    \begin{bmatrix} 
        a \cdot d && a \cdot e \\
        b \cdot d && \mathbf{b \cdot e} \\
        c \cdot d && c \cdot e
    \end{bmatrix}
\]

Where \(a \cdot b\) denotes the `dot product' between a row and a column.

\subsection{Matrix transpose}

The transpose operator is analogous to that of vectors

\[
    \begin{pmatrix}
        a_1 & a_2 & \cdots & a_n
    \end{pmatrix}^t
    =
    \begin{pmatrix}
        a_1 \\
        a_2 \\
        \vdots \\
        a_n
    \end{pmatrix}
\]

This is actually a special case, where we apply the transpose operator to a matrix column.

The transpose of a generic \(n \times m\) matrix results in a \(m \times n\) one.

\[
    {\left(A_{ij}\right)}^t=A_{ji}
\]

The transpose of a matrix is just a flipped version of the original matrix.
We can transpose a matrix by switching its rows with its columns.
The original rows become the new columns and the original columns become the new rows.

\[
    {\begin{bmatrix} 
        a_{1,1} & a_{1,2} & \cdots & a_{1,m} \\
        a_{2,1} & a_{2,2} & \cdots & a_{2,m} \\
        \vdots  & \vdots  & \ddots & \vdots  \\
        a_{n,1} & a_{n,2} & \cdots & a_{n,m} 
    \end{bmatrix}}^t
    =
    \begin{bmatrix} 
        a_{1,1} & a_{2,1} & \cdots & a_{m,1} \\
        a_{1,2} & a_{2,2} & \cdots & a_{m,2} \\
        \vdots  & \vdots  & \ddots & \vdots  \\
        a_{1,n} & a_{2,n} & \cdots & a_{m,n} 
    \end{bmatrix}
\]

\subsection{Matrix inverse}

An \(n \times n\) matrix \(A\) is invertible (also nonsingular or nondegenerate),
if there exists an \(n \times n\) matrix \(A^{-1}\) such that

\[
    AA^{-1}=I_n
\]

If \(\det(A) = 0\), meaning that the matrix collapses space into a lower dimension,
thus resulting in a loss of information, the matrix is not invertible.
Otherwise, the matrix has an inverse and it is unique.

This is equivalent to saying that if \(A\vec{v}=0\) for some non-zero vector \(\vec{v}\),
then \(A\) has no inverse.

\pagebreak

\section{Types of matrices}

\subsection{Identity matrix}

The identity matrix is a square matrix with ones on the main diagonal and zeros elsewhere.
This matrix is denoted as \(I_n\) where \(n\) is the length of the side.

\[
    I_n=
    \begin{bmatrix}
        1 & 0 & 0 & \cdots & 0 \\
        0 & 1 & 0 & \cdots & 0 \\
        0 & 0 & 1 & \cdots & 0 \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & 0 & \cdots & 1 \\
    \end{bmatrix}
\]

When \(I_n\) is applied to a matrix or vector, the matrix or vector remains the same.

% definition using the Kronecker delta

\subsection{Matrix adjoint}

Given a complex matrix \(A\) we form the transpose and then take the complex conjugate of every element in it.

\[
    A^\dagger \equiv {\left(A^t\right)}^{*}
\]

\subsection{Hermitian Matrix}

A matrix \(M\) is called Hermitian if it is equal to its adjoint.

\[
    M^\dagger = M
\]

\subsection{Vandermonde matrix}

A Vandermonde matrix is a matrix with the terms of a geometric progression in each row or column. \\
Here is an \(m \times n\) matrix.

\[
    V =
    \begin{bmatrix}
        1 & a_1 & a_1^2 & \cdots & a_1^{n-1} \\
        1 & a_2 & a_2^2 & \cdots & a_2^{n-1} \\
        1 & a_3 & a_3^2 & \cdots & a_3^{n-1} \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        1 & a_m & a_m^2 & \cdots & a_m^{n-1} \\
    \end{bmatrix}
\]

\pagebreak

\subsection{Determinants}

Every square matrix has a scalar called the determinant. \\
The determinant is the factor by which the area of any shape (or volume and so on) changes after the transformation is applied.
If the transformation inverts the orientation of the space, the determinant is negative.

To compute the determinant of a matrix, we must define the minor of a matrix element. \\
The minor of a matrix element is the part of the matrix remaining after excluding the row and the column containing that particular element.

\[
    \text{minor of }b=
    \stackinset{c}{}{c}{1\baselineskip}{\rule{4.25\baselineskip}{0.5pt}}{
    \stackinset{c}{0\baselineskip}{c}{}{\rule{0.5pt}{3.5\baselineskip}}{
    \begin{bmatrix}
        a & b & c \\
        d & e & f \\
        g & h & i
    \end{bmatrix}}}
    =
    \begin{vmatrix}
        d & f \\
        g & i
    \end{vmatrix}
\]

To compute the determinant of a matrix, for each element of a row or column, we multiply the element by the determinant of its minor.
We then sum all of these values, but every even term has a negative sign.
This is a recursive operation for \(n > 2\).

For example, given a \(2 \times 2\) matrix \(A\)

\[
    A=
    \begin{bmatrix}
        a & b \\
        c & d
    \end{bmatrix}
\]

its determinant is defined as

\[
    \det(A)=
    \begin{vmatrix}
        a & b \\
        c & d
    \end{vmatrix}
    \equiv ad-bc
\]

For the \(3 \times 3\) case

\begin{align*}
    B&=
    \begin{bmatrix}
        a & b & c \\
        d & e & f \\
        g & h & i
    \end{bmatrix}
    \\
    \det(B)=
    \begin{vmatrix}
        a & b & c \\
        d & e & f \\
        g & h & i
    \end{vmatrix}
    &=
    a \begin{vmatrix}
        e & f \\
        h & i
    \end{vmatrix}
    -b \begin{vmatrix}
        d & f \\
        g & i
    \end{vmatrix}
    +c \begin{vmatrix}
        d & e \\
        g & h
    \end{vmatrix}
\end{align*}

In general, we can define the determinant of a martrix using the Laplace expansion.

\[
    \det(A)=\sum_{j=1}^{n}{(-1)}^{i+j}A_{ij}\det(\text{minor of }A_{ij})
\]

where \(i\) is any row.

Instead of expanding the series along a row we could expand it along any column. \\
The result is always the same no matter what row or what column.

The determinant has some properties, for instance

\begin{align*}
    \det(A^t)&=\det(A) \\
    \det(AB)&=\det(A)\det(B)
\end{align*}

\pagebreak

\section{Column space}

The column space (or range or image) of a matrix is the
set of all possible vectors that can be generated using the transformation.

\section{Rank of a matrix}

The rank of a matrix \(M\) is the dimension of its column space.
A rank \(n\) means that every vector after the transformation of \(M\)
is projected in the \(n\)-dimension.

A matrix is full rank if its rank is equal to the number of its columns, meaning
the dimension does not change.
A matrix is full rank iff \(\vec{v}=\vec{0}\) is the only vector such that \(M\vec{v}=\vec{0}\).

Note that non-square matrices may still be full rank even if the vectors end up in a lower dimension.
If there is no loss of information the matrix is still full rank.
If \(M\) has dimensions \(m \times n\), then it is full rank if \(\text{Rank}(M)=\min(n,m)\).

\section{Null space}

The null space or the kernel of a matrix is the set of all the vectors that land on the origin
after the transformation.
The kernel is the linear subspace of the domain of the transformation which is mapped to \(\vec{0}\).

\pagebreak

\section{System of linear equations}

\subsection{Interpretation}

A system of linear equations
\begin{align*}
    a_1x + b_1y + c_1z &= d_1 \\
    a_2x + b_2y + c_2z &= d_2 \\
    a_3x + b_3y + c_3z &= d_3 \\
\end{align*}

Can be represented by a matrix multiplication \(M\vec{x}=\vec{d}\)

\[
    \begin{bmatrix} 
        a_1 && b_1 && c_1 \\
        a_2 && b_2 && c_2 \\
        a_3 && b_3 && c_3
    \end{bmatrix}
    \begin{bmatrix} 
        x \\ y \\ z
    \end{bmatrix}
    =
    \begin{bmatrix} 
        d_1 \\ d_2 \\ d_3
    \end{bmatrix}
\]

The geometrical interpretation is to find the vector \(\vec{x}\)
such that when the matrix \(M\) is applied to it, the resulting vector is \(\vec{d}\).

\subsection{Cramer's rule}

Consider a system of \(n\) equations and unknowns
\[
    A\vec{x}=\vec{b}
\]
The solution is given by
\[
    \vec{x}_i = \frac{\det(A_i)}{\det(A)}
\]
where \(A_i\) is formed by replacing the \(i\)-th column
of \(A\) by \(\vec{b}\).

\pagebreak

\section{Change of basis}

\subsection{Change of basis of a vector}

Given a basis \(\mathcal{B}_1\) and \(\mathcal{B}_2\),
there exists a matrix \(M\) such that \(M\vec{v}\) translates
the vector from \(\mathcal{B}_1\) to \(\mathcal{B}_2\).

The matrix \(M\) is formed by the columns of \(\mathcal{B}_1\) but written in
\(\mathcal{B}_2\).

The operation \(M^{-1}\vec{v}\) translates
the vector from \(\mathcal{B}_2\) to \(\mathcal{B}_1\).

\subsection{Change of basis of a linear transformation}

If \(T_1\) is a transformation in \(\mathcal{B}_1\), we can find another
transformation \(T_2\) that is similar to \(T_1\) but works for the basis \(\mathcal{B}_2\).

Instead of computng \(T_1\vec{v}\), we first multiply \(\vec{v}\) by the
change of basis matrix \(M\), then apply \(T_1\) and finally apply the inverse change of basis matrix \(M^{-1}\).
\[
    T_2 = M^{-1}T_1M
\]

\section{Eigenvalues and eigenvectors}

The eigenvectors \(\vec{v}\) and eigenvalues \(\lambda\) of a matrix \(M\)
are values such that
\[
    M\vec{v} = \lambda \vec{v}
\]

\pagebreak

\section{Rotational matrix}

The standard rotation matrix in two dimensions has the following form:

\[
    R(\theta)=
    \begin{bmatrix} 
        \cos\theta && -\sin\theta \\
        \sin\theta && \cos\theta
    \end{bmatrix}
\]

Multiplying a vector \(\vec{p}\in \mathbb{R}^2\) by \(R(\theta)\) will rotate the point around the origin depending on \(\theta\).

\[
    \vec{p}=
    \begin{pmatrix} 
        x \\
        y
    \end{pmatrix}
\]

We can multiply the rotational matrix by the column vector to obtain the new coordinates

\[
    \begin{bmatrix} 
        x' \\
        y'
    \end{bmatrix}
    =
    \begin{bmatrix} 
        \cos\theta && -\sin\theta \\
        \sin\theta && \cos\theta
    \end{bmatrix}
    \cdot
    \begin{bmatrix} 
        x \\
        y
    \end{bmatrix}
    =
    \begin{bmatrix} 
        x\cos\theta - y\sin\theta \\
        x\sin\theta + y\cos\theta
    \end{bmatrix}
\]

Note: the matrix must be positioned on the right of the vector, otherwise the dimensions would no longer be compatible.

The rotation matrices for a three-dimensional point are

\begin{align*}
    R_x(\theta)&=
    \begin{bmatrix} 
        1 & 0 & 0 \\
        0 & \cos\theta & -\sin\theta \\
        0 & \sin\theta & \cos\theta
    \end{bmatrix}
    \\
    R_y(\theta)&=
    \begin{bmatrix} 
        \cos\theta & 0 & \sin\theta \\
        0 & 1 & 0 \\
        -\sin\theta & 0 & \cos\theta
    \end{bmatrix}
    \\
    R_z(\theta)&=
    \begin{bmatrix} 
        \cos\theta & -\sin\theta & 0 \\
        \sin\theta & \cos\theta & 0 \\
        0 & 0 & 1
    \end{bmatrix}
\end{align*}

\end{document}