\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,stackengine}
\usepackage{amssymb}
\usepackage{parskip}
\usepackage{fullpage}
\usepackage{hyperref}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    urlcolor=blue,
    pdftitle={Matrices},
    pdfpagemode=FullScreen,
}

\title{Matrices}
\author{Paolo Bettelini}
\date{}

\newcommand{\braket}[1]{\left\langle#1\right\rangle}
\newcommand{\innerprod}[2]{\braket{#1\,|\,#2}}

\stackMath{}

\begin{document}

\maketitle
\tableofcontents
\pagebreak

\section{Definition}

A matrix is a rectangular array of numbers or variables. \\
Each matrix has a particular size espressed as \(n \times m\). \\
A generic matrix could look like

\[
A = \begin{bmatrix} 
        a_{1,1} & a_{1,2} & \cdots & a_{1,m} \\
        a_{2,1} & a_{2,2} & \cdots & a_{2,m} \\
        \vdots  & \vdots  & \ddots & \vdots  \\
        a_{n,1} & a_{n,2} & \cdots & a_{n,m} 
    \end{bmatrix}
\]

\section{Operations with matrices}

\subsection{Matrix multiplication}

Matrix multiplication is not a communitative operation.

\[
    AB\neq BA
\]

However it is an associative operation.

\[
    A(BC)=(AB)C
\]

Matrix multiplication can be computed only if the numer of columns of the first matrix is equal to the number of rows of the other matrix. \\
This menas that we can compute the product of a \(n \times m\) matrix with a \(i \times j\) matrix, only if \(m=i\). \\
The resulting product is a matrix with \(n\) rows and \(j\) columns.

\[
    (n \times m) \cdot (m \times j) = (n \times j)
\]

If we multiply a \(1 \times n\) matrix by a \(n \times 1\) one, the result is a \(1 \times 1\) matrix or a scalar.

\[
    \begin{bmatrix} 
        a_1 & a_2 & \cdots & a_n
    \end{bmatrix}
    \cdot
    \begin{bmatrix} 
        b_1 \\
        b_2 \\
        \vdots \\
        b_n
    \end{bmatrix}
    = \sum_{k=1}^{n} a_k b_k = a_1 b_1 + a_2 b_2 + \cdots + a_n b_n
\]

This operation is pretty much the same as the dot product in vectors.

For bigger matrices, we apply the same `dot product' between every row and every column. \\
Here is an example:

\[
    \begin{bmatrix} 
        a_1 && a_2 && a_3 && a_4 \\
        \mathbf{b_1} && \mathbf{b_2} && \mathbf{b_3} && \mathbf{b_4} \\
        c_1 && c_2 && c_3 && c_4
    \end{bmatrix}
    \cdot
    \begin{bmatrix} 
        d_1 && \mathbf{e_1} \\
        d_2 && \mathbf{e_2} \\
        d_3 && \mathbf{e_3} \\
        d_4 && \mathbf{e_4}
    \end{bmatrix}
    =
    \begin{bmatrix} 
        a \cdot d && a \cdot e \\
        b \cdot d && \mathbf{b \cdot e} \\
        c \cdot d && c \cdot e
    \end{bmatrix}
\]

Where \(a \cdot b\) denotes the `dot product' between a row and a column.

\pagebreak

\subsection{Matrix transpose}

The transpose operator is analogous to that of vectors

\[
    \begin{pmatrix}
        a_1 & a_2 & \cdots & a_n
    \end{pmatrix}^t
    =
    \begin{pmatrix}
        a_1 \\
        a_2 \\
        \vdots \\
        a_n
    \end{pmatrix}
\]

This is actually a special case, where we apply the transpose operator to a matrix column.

The transpose of a generic \(n \times m\) matrix results in a \(m \times n\) one.

\[
    {\left(A_{ij}\right)}^t=A_{ji}
\]

The transpose of a matrix is just a flipped version of the original matrix.
We can transpose a matrix by switching its rows with its columns.
The original rows become the new columns and the original columns become the new rows.

\[
    {\begin{bmatrix} 
        a_{1,1} & a_{1,2} & \cdots & a_{1,m} \\
        a_{2,1} & a_{2,2} & \cdots & a_{2,m} \\
        \vdots  & \vdots  & \ddots & \vdots  \\
        a_{n,1} & a_{n,2} & \cdots & a_{n,m} 
    \end{bmatrix}}^t
    =
    \begin{bmatrix} 
        a_{1,1} & a_{2,1} & \cdots & a_{m,1} \\
        a_{1,2} & a_{2,2} & \cdots & a_{m,2} \\
        \vdots  & \vdots  & \ddots & \vdots  \\
        a_{1,n} & a_{2,n} & \cdots & a_{m,n} 
    \end{bmatrix}
\]

\subsection{Identity matrix}

The identity matrix is a square matrix with ones on the main diagonal and zeros elsewhere.
This matrix is denoted as \(I_n\) where \(n\) is the length of the side.

\[
    I_n=
    \begin{bmatrix}
        1 & 0 & 0 & \cdots & 0 \\
        0 & 1 & 0 & \cdots & 0 \\
        0 & 0 & 1 & \cdots & 0 \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & 0 & \cdots & 1 \\
    \end{bmatrix}
\]

When \(I_n\) is applied to a matrix or vector, the matrix or vector remains the same.

% definition using the Kronecker delta

\subsection{Matrix inverse}

An \(n \times n\) matrix \(A\) is invertible (also nonsingular or nondegenerate),
if there exists an \(n \times n\) matrix \(A^{-1}\) such that

\[
    AA^{-1}=I_n
\]

\begin{itemize}
    \item If \(A\vec{v}=0\) for some non-zero vector \(\vec{v}\), then \(A\) has no inverse.
    \item If \(A\) is invertible, \(\det(A)\neq 0\).
    \item If \(A\vec{v}=0\) for some non-zero vector \(\vec{v}\), then \(\det(A)=0\).
\end{itemize}

\pagebreak

\subsection{Matrix adjoint}

Given a complex matrix \(A\) we form the transpose and then take the complex conjugate of every element in it.

\[
    A^\dagger \equiv {\left(A^t\right)}^{*}
\]

\subsection{Hermitian Matrix}

A matrix \(M\) is called Hermitian if it is equal to its adjoint.

\[
    M^\dagger = M
\]

\subsection{Vandermonde matrix}

A Vandermonde matrix is a matrix with the terms of a geometric progression in each row or column. \\
Here is an \(m \times n\) matrix.

\[
    V =
    \begin{bmatrix}
        1 & a_1 & a_1^2 & \cdots & a_1^{n-1} \\
        1 & a_2 & a_2^2 & \cdots & a_2^{n-1} \\
        1 & a_3 & a_3^2 & \cdots & a_3^{n-1} \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        1 & a_m & a_m^2 & \cdots & a_m^{n-1} \\
    \end{bmatrix}
\]

\pagebreak

\subsection{Determinants}

Every square matrix has a scalar called the determinant.

To compute the determinant of a matrix, we must define the minor of a matrix element. \\
The minor of a matrix element is the part of the matrix remaining after excluding the row and the column containing that particular element.

\[
    \text{minor of }b=
    \stackinset{c}{}{c}{1\baselineskip}{\rule{4.25\baselineskip}{0.5pt}}{
    \stackinset{c}{0\baselineskip}{c}{}{\rule{0.5pt}{3.5\baselineskip}}{
    \begin{bmatrix}
        a & b & c \\
        d & e & f \\
        g & h & i
    \end{bmatrix}}}
    =
    \begin{vmatrix}
        d & f \\
        g & i
    \end{vmatrix}
\]

To compute the determinant of a matrix, for each element of a row or column, we multiply the element by the determinant of its minor.
We then sum all of these values, but every even term has a negative sign.
This is a recursive operation for \(n > 2\).

For example, given a \(2 \times 2\) matrix \(A\)

\[
    A=
    \begin{bmatrix}
        a & b \\
        c & d
    \end{bmatrix}
\]

its determinant is defined as

\[
    \det(A)=
    \begin{vmatrix}
        a & b \\
        c & d
    \end{vmatrix}
    \equiv ad-bc
\]

For the \(3 \times 3\) case

\begin{align*}
    B&=
    \begin{bmatrix}
        a & b & c \\
        d & e & f \\
        g & h & i
    \end{bmatrix}
    \\
    \det(B)=
    \begin{vmatrix}
        a & b & c \\
        d & e & f \\
        g & h & i
    \end{vmatrix}
    &=
    a \begin{vmatrix}
        e & f \\
        h & i
    \end{vmatrix}
    -b \begin{vmatrix}
        d & f \\
        g & i
    \end{vmatrix}
    +c \begin{vmatrix}
        d & e \\
        g & h
    \end{vmatrix}
\end{align*}

In general, we can define the determinant of a martrix using the Laplace expansion.

\[
    \det(A)=\sum_{j=1}^{n}{(-1)}^{i+j}A_{ij}\det(\text{minor of }A_{ij})
\]

where \(i\) is any row.

Instead of expanding the series along a row we could expand it along any column. \\
The result is always the same no matter what row or what column.

The determinant has some properties, for instance

\begin{align*}
    \det(A^t)&=\det(A) \\
    \det(AB)&=\det(A)\det(B)
\end{align*}

\pagebreak

\section{Linear Transformation}

\subsection{Definition}

Multiplying a vector by a matrix produces another vector. This is a linear transformation. \\
A linear transformation \(T\) moves a vector around the vector space and/or to another space.

\[
    \mathcal{V} \xrightarrow{T} \mathcal{W}
\]

The linear transformation has the following proprierties:

\begin{enumerate}
    \item \(T(c\vec{a})=cT\vec{a}\)
    \item \(T(\vec{a} + \vec{b}) = T\vec{a} + T\vec{b}\)
\end{enumerate}

where \(c\) is a scalar of the vector space.

\subsection{The basis}

Given a basis for a vector space

\[
    \mathcal{B}=\{\vec{b}_1, \vec{b}_2, \ldots, \vec{b}_n\}
\]

we can expand a vector \(\vec{a}\) along this basis

\[
    \vec{a} = \sum_{k=1}^{n} \alpha_k \mathcal{B}_k
\]

We then apply a transformation \(T\) to the vector \(\vec{a}\) and use the proprierties of linear transformations

\[
    T\vec{a}
    = T\sum_{k=1}^{n} \alpha_k \mathcal{B}_k
    = \sum_{k=1}^{n} \alpha_k T\mathcal{B}_k
\]

This tells us that we only need \(T\mathcal{B}=\{T\vec{b}_1, T\vec{b}_2, \ldots, T\vec{b}_n\}\)
to determine \(T\) applied to any vector on the vector space. Also, \(T\mathcal{B}\) completely determines the transformation \(T\).

\pagebreak

\subsection{Rotational matrix}

The standard rotation matrix in two dimensions has the following form:

\[
    R(\theta)=
    \begin{bmatrix} 
        \cos\theta && -\sin\theta \\
        \sin\theta && \cos\theta
    \end{bmatrix}
\]

Multiplying a vector \(\vec{p}\in \mathbb{R}^2\) by \(R(\theta)\) will rotate the point around the origin depending on \(\theta\).

\[
    \vec{p}=
    \begin{pmatrix} 
        x \\
        y
    \end{pmatrix}
\]

We can multiply the rotational matrix by the column vector to obtain the new coordinates

\[
    \begin{bmatrix} 
        x' \\
        y'
    \end{bmatrix}
    =
    \begin{bmatrix} 
        \cos\theta && -\sin\theta \\
        \sin\theta && \cos\theta
    \end{bmatrix}
    \cdot
    \begin{bmatrix} 
        x \\
        y
    \end{bmatrix}
    =
    \begin{bmatrix} 
        x\cos\theta - y\sin\theta \\
        x\sin\theta + y\cos\theta
    \end{bmatrix}
\]

Note: the matrix must be positioned on the right of the vector, otherwise the dimensions would no longer be compatible.

The rotation matrices for a three-dimensional point are

\begin{align*}
    R_x(\theta)&=
    \begin{bmatrix} 
        1 & 0 & 0 \\
        0 & \cos\theta & -\sin\theta \\
        0 & \sin\theta & \cos\theta
    \end{bmatrix}
    \\
    R_y(\theta)&=
    \begin{bmatrix} 
        \cos\theta & 0 & \sin\theta \\
        0 & 1 & 0 \\
        -\sin\theta & 0 & \cos\theta
    \end{bmatrix}
    \\
    R_z(\theta)&=
    \begin{bmatrix} 
        \cos\theta & -\sin\theta & 0 \\
        \sin\theta & \cos\theta & 0 \\
        0 & 0 & 1
    \end{bmatrix}
\end{align*}

\subsection{Unitary operators}

A linear transformation \(U\) is unitary if it preserves inner products

\[
    \innerprod{U\vec{a}}{U\vec{b}} = \innerprod{\vec{a}}{\vec{b}}
\]

This statement implies that the length of the vector is also preserved

\[
    ||U\vec{a}|| = ||\vec{a}||
\]

Operations such as rotation or phase change are unitarx operators.

\end{document}

% https://lapastillaroja.net/wp-content/uploads/2016/09/Intro_to_QC_Vol_1_Loceff.pdf