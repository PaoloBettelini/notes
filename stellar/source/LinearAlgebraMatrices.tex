\documentclass[preview]{standalone}

\usepackage{amsmath,stackengine}
\usepackage{amssymb}
\usepackage{parskip}
\usepackage{fullpage}
\usepackage{hyperref}
\usepackage{stellar}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    urlcolor=blue,
    pdftitle={Matrices},
    pdfpagemode=FullScreen,
}

\begin{document}

\title{Matrices}
\id{matrices}
\genpage

\section{Definition}

\includesnpt{linearalgebra-matrix-definition}

% TODO linear transformation

\section{Elementary row operations}

\includesnpt{linearalgebra-elementary-row-operations}

\begin{snippet}{systems-of-linear-equations}
\section{Systems of linear equations}

A system of linear equations
\begin{align*}
    a_1x + b_1y + c_1z &= d_1 \\
    a_2x + b_2y + c_2z &= d_2 \\
    a_3x + b_3y + c_3z &= d_3 \\
\end{align*}

Can be represented by a matrix multiplication \(M\vec{x}=\vec{d}\)

\[
    \begin{bmatrix} 
        a_1 && b_1 && c_1 \\
        a_2 && b_2 && c_2 \\
        a_3 && b_3 && c_3
    \end{bmatrix}
    \begin{bmatrix}
        x \\ y \\ z
    \end{bmatrix}
    =
    \begin{bmatrix}
        d_1 \\ d_2 \\ d_3
    \end{bmatrix}
\]

The geometrical interpretation is to find the vector \(\vec{x}\)
such that when the matrix \(M\) is applied to it, the resulting vector is \(\vec{d}\).

We may represent the whole system just by
\[
    \begin{bmatrix} 
        a_1 && b_1 && c_1 && d_1 \\
        a_2 && b_2 && c_2 && d_2 \\
        a_3 && b_3 && c_3 && d_3
    \end{bmatrix}
\]
\end{snippet}

\subsection{Using elementary row operations}

\begin{snippet}{linearalgebra-expl1}
Applying \snippetref[linearalgebra-elementary-row-operations][elementary row operations]
does not change the solution of the linear system.
\end{snippet}

\includesnpt{linearalgebra-lin-sys-sol-amount}

\begin{snippet}{linearalgebra-expl2}
By applying these operations, our goal is to make the system matrix
look like the following:
\[
    \begin{bmatrix} 
        1 && 0 && 0 && e_1 \\
        0 && 1 && 0 && e_2 \\
        0 && 0 && 1 && e_3
    \end{bmatrix}
\]
which is the implicit solution
\(x=e_1\), \(y=e_2\) and \(z=e_3\).
If this is possible, then this is the only solution to our system.

\vspace{.25cm}

If it is possible to create a row whose elements are all \(0\)s,
then there are infinitely many solutions, because
there are infinitely many solutions for the equation
\(0x_1+0x_2+\cdots+0x_n = 0\).

\vspace{.25cm}

If it is possible to create a whose elements elements
are all \(0\)s expect for the last one there are zero solutions,
because there are zero solutions for the equation
\(0x_1+0x_2+\cdots+0x_n = a\) with \(a \neq 0\).
\end{snippet}

\subsubsection{Examples}

\includesnpt{linearalgebra-matrix-linear-system-1-sol-example-1}
\includesnpt{linearalgebra-matrix-linear-system-inf-sol-example-1}
\includesnpt{linearalgebra-matrix-linear-system-0-sol-example-1}

% the set of all m times n matrices?

% the properties like commutativity and associativity
% are there only if the elements have it

\section{Definitions}

\includesnpt{linearalgebra-zero-matrix-definition}

\includesnpt{linearalgebra-matrix-addition-definition}

\includesnpt{linearalgebra-matrix-scalar-multiplication-definition}

% TODO properties of addition
% A+0_{m,n} = A
% A+B = B + A
% 0A = 0_{m,n}
% A+(-A)= 0_{m,n}
% (A+B)+C = A + (B + C)
% 1A=A
% (a+b) A = aA + bA
% a(A+B) = aA+aB
% a(bA) = (ab)A

\includesnpt{linearalgebra-matrix-multiplication-definition}

\begin{snippet}{linearalgebra-expl3}
This means applying a `dot product' between every row and every column. \\

\[
    \begin{bmatrix} 
        a_1 && a_2 && a_3 && a_4 \\
        \mathbf{b_1} && \mathbf{b_2} && \mathbf{b_3} && \mathbf{b_4} \\
        c_1 && c_2 && c_3 && c_4
    \end{bmatrix}
    \cdot
    \begin{bmatrix} 
        d_1 && \mathbf{e_1} \\
        d_2 && \mathbf{e_2} \\
        d_3 && \mathbf{e_3} \\
        d_4 && \mathbf{e_4}
    \end{bmatrix}
    =
    \begin{bmatrix} 
        a \cdot d && a \cdot e \\
        b \cdot d && \mathbf{b \cdot e} \\
        c \cdot d && c \cdot e
    \end{bmatrix}
\]

Where \(a \cdot b\) denotes the `dot product' between a row and a column.

This operation is not commutative, but it is associative.
\end{snippet}

\includesnpt{linearalgebra-kronecker-delta-definition}

\includesnpt{linearalgebra-identity-matrix-definition}

\includesnpt{linearalgebra-kronecker-delta-sifting-property}

\begin{snippet}{linearalgebra-expl4}
This is given by the fact that \(\delta_{i,k}\)
will be \(1\) only when \(i = k\).
\end{snippet}

% TODO properties fo mul and proofs
% A0_{n,p} = 0_{m,p}
% 0_{p,m}A=0_{p,n}

\includesnpt{linearalgebra-matrix-premultiplication}
\includesnpt{linearalgebra-matrix-postmultiplication}

\includesnpt{linearalgebra-matrix-identity-postmultiplication}
\includesnpt{linearalgebra-matrix-identity-postmultiplication-proof}
\includesnpt{linearalgebra-matrix-identity-premultiplication}
\includesnpt{linearalgebra-matrix-identity-premultiplication-proof}

\includesnpt{linearalgebra-polynomial-of-a-matrix-definition}

\includesnpt{linearalgebra-matrix-exponentiation-definition}
\includesnpt{linearalgebra-matrix-exponentiation-example-1}
\includesnpt{linearalgebra-matrix-exponentiation-example-2}

\includesnpt{linearalgebra-matrix-inverse-definition}
\includesnpt{linearalgebra-invertible-matrix-definition}

\includesnpt{linearalgebra-uniqueness-of-matrix-inverse}
\includesnpt{linearalgebra-uniqueness-of-matrix-inverse-proof}

\includesnpt{linearalgebra-product-rule-of-inverse-matrices}
\includesnpt{linearalgebra-product-rule-of-inverse-matrices-proof}

\includesnpt{linearalgebra-involution-rule-for-matrix-inverse}
\includesnpt{linearalgebra-involution-rule-for-matrix-inverse-proof}

\includesnpt{linearalgebra-matrix-left-and-write-inverses}
\includesnpt{linearalgebra-inverse-of-a-non-square-matrix}
\includesnpt{linearalgebra-inverse-of-a-square-matrix}

\includesnpt{linearalgebra-matrix-invertibility}
\begin{snippet}{linearalgebra-expl5}
If \(\det(M) = 0\), meaning that the matrix collapses space into a lower dimension,
thus resulting in a loss of information, the matrix is not invertible.

This is equivalent to saying that if \(M\vec{v}=0\) for some non-zero vector \(\vec{v}\),
then \(M\) has no inverse.
\end{snippet}

% TODO: commutability matrix theorem

\section{Reduced Row Echelon Forms}

\end{document}
